10/7/22: Meeting 6:
	- Fixed compile of LITMUS by using server instead of LinuxLabs
	Weekly Notes:
	- Got LITMUS installed! 
	- SSH key authentication is oddly slow. 
	- Large computations sometimes drop ssh connection, especially on SCHED_RR tasks. Possilby bandwidth issues? 
		- Fixed by lowering matrix size. 
	- Seemingly much slower to run in LITMUS
	- Though much much faster with MultiCore than single core on LITMUS, compared to base. 

9/28/22: Meeting 5:
      - Reservations in LITMUS are treated seperatley in cores 
            - Not sure if its through system calls or VFS 
      - Argument in paper- Linux Capabilities are primitive compared to SEL4. We make them comperable 
      - We could do something more powerful than LITMUS by using cgroups 
      - Cross compile LITMUS on LinuxLab's and make 2 images on SD card like we did in CSE422/522 
            - Studio 1 from CSE522 
      - Can claim meaningful prototype if get Linux cgroup implementation similar to LITMUS 
      - RTSS (theory) or RTAS (applied) Conference Paper; MSOFT also an option. 
      - 3-5 pages for Masters Project (probably already have); More for Thesis with ideas polished out more

9/21/22: Meeting 4 (virtual):
      - /kernel/sched/rt.c - Checks for valid input
      - Check each core.
      - Start with fixed priority scheduling 
      - Trace out how cpu.max goes 
      - Look at /kernel/sched/fair.c to see how CFS implements bandwitch constraints (for knowledge only)
      - To see:
            - ft.c to see how existing constraint is implemented and enforced. 
            - fair.c to see how bandwidth constraints are used
            - deadline.c to see bandwidth constrains on a per core basis 
      - Marion to send out email with summary 

9/14/22: Meeting 3:
      - Linux Real time group scheduling isn't turned on by default 
            - Limitations: Total utailiziation bound is 1 regardless of MultiCore System 
            CPU Set and bandwidth constraint only for CFS not RealTime 
      - idea: Add this to cgroups v2
      - write the minimal code to levereverge cgroup v1 code for cgroup v2 usage. 
      - Look at cgroup v1 code and port it to cgroup v2 code. 


9/2/22: Second Meeting:
      - Innercomponent requests
      - Fixed priority scheduling on MultiCore systems isn't great, so we will do that
            - Kernel Mechanism 
      - Utaliziation should be less than 100% on each processor. Right now, overall system 
            is limited to 100%, but we want each processor to be limited to 100% 
            so that overall you can have over 100% Utaliziation for multicore systems 
      - Enforcment Mechanism for management of constraint utailiziation 
      - Targeting RTNS Paper --> What systems doctorial research looks like 
            - Chaning the way people think about how things work
            

9/2/22: First Fall22 Meeting:
      - Accounting system to keep track of usage of cgroup
            Server program joins client. Server will have residual work 
            and we will see if statics persists or is reset. 
            Basic test to start moving forward 
      - Is there a way to enforce that server won't go join a cgroup with more bandwidth? 
            - Stack abstraction: can only move into and out of specific cgroup
      -  2 pools of threads that each have own capability and trust within them, 
            what if they want to trust eachother
      - Figure out how Linux actually works. Then, use this to see what naturally makes sense. 
            Will likely need Kernel Module to account the open/ close of scope. 
      - Project Title: Rational Enforcment Kernel Structure using cgroups 
      - Design use of manifest constants similar to syscall table but for capbility numbers,
            which are implemented as bit mask
      - Linux vs sel4 --> Capibilies in Linux are binary (have it or not), while sel4 have more options
            Could namespacing be used to add attributes of if you have the capibility 
            Semantically bring sel4 to Linux. Syntactically C++
      
      - Comitte: Chris Gill, Ning Zhang, Silvia Zhang, possibly 


Meeting Notes (5/5/22):
   - Look into how cgroup v2 supports threads https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#threads
   - cgroup global
      -- Threaded -> server and all clients
   1.) Leaderboard table for identites and roles 
   2.) Admission control/ broker for client/server registration with cgroups created and associated and split/ merged accordingly 
   Uses:
      - elastic/mc/hierarchial CPU
      - access control (i.e. files)
      - Dedicated/ Shared Memory
      Ex: Camera takes pictures and can change image (i.e. matrix) size accordingly

Updates (5/4/22):
   - Fixed Dynamic Allocation bug, though omiited struct in favor of access to Pointer directly, using offset
   - Added option to use normal, naive MM or cache-optimal version (conditional compilation so that doesn't effect performance)
      - Issues with valgrind on Pi, so went with wrapper for CPU hardware performance counters
         - Tested on LinuxLab first as well using valgrind, then with perf tool
      - Not much cache performance difference as expected. Why?
   - Implemented client setup cgroup and server thread joins that cgroup
      when it connects, basicially inventing a new pattern.
	 How does deleting a cgroup work, or is it left to OS to cleanup?
         However, PID is the same between threads since we use pthread_create. 
         How to address this, or is it okay?   


Question (4/20/22): 
   - Parameters for SCHED_DEADLINE?? 
      - Wait for periodic timer version to see interaction 
   - Pointer use in shared memory??    [FIXED 5/3/22]
      - Don't use calloc- ftruncate and mmap with size of this.
   - What is point of cache optimal Matrix Multiply if we want workload? 
         Could use cachegrind tool available in valgrind
         Cache Memory 2 from CSE361S Slides
         Compare cache performance vs scheduling policy vs requests that come in 
         Gill: Single thread
         Marion: Performance monitoring from 522 Studio 3 to see cache hits/misses 
         

         Ended up using performance stats (which uses hardware performance counters of CPU) command: 
            https://stackoverflow.com/questions/10082517/simplest-tool-to-measure-c-program-cache-hit-miss-and-cpu-time-in-linux
            https://perf.wiki.kernel.org/index.php/Tutorial
            
         $ perf stat -e cache-misses -e cache-references ./cache_optimal_dense_mm 300

         Results for Original dense_mm for 500 size matrix:
               63,300,177      cache-misses:u            # 2.094 % of all cache refs
               3,023,069,371      cache-references:u

               8.898626738 seconds time elapsed

               8.861466000 seconds user
               0.030004000 seconds sys

         Results for Cache Optimal dense_mm for 500 size matrix:
               817,476      cache-misses:u            # 0.032 % of all cache refs
               2,524,564,128      cache-references:u

               5.119169061 seconds time elapsed

               5.092216000 seconds user
               0.020008000 seconds sys
       

         Results for Original dense_mm for 1000 size matrix:
               1,143,874,967      cache-misses:u        # 4.748 % of all cache refs
               24,092,088,964      cache-references:u

               87.931522410 seconds time elapsed

               87.771355000 seconds user
               0.070025000 seconds sys

         Results for Cache Optimal dense_mm for 1000 size matrix:
               7,030,832      cache-misses:u            # 0.035 % of all cache refs
               20,098,076,481      cache-references:u

               42.470107905 seconds time elapsed

               42.389324000 seconds user
               0.039989000 seconds sys


         Overall:
            ~ 60X Less Miss Rate for cache optimal versus original on 500 size matrix
            ~ 135X Less Miss Rate for cache optimal versus original on 1000 size matrix


Notes (4/14):
   - SCHED_DEADLINE add to flag set and see how it performs as well 
   - Change to dynamic allocation [DONE]
   - Possibly change matrix multiply to Cache-Optimal version 
   - Client is in a Cgroup, and Server thread joins that Cgroup when it links (inventing new pattern)
   - Server in own Cgroup, then above, and compare performance 
   - Marion: Hierarchy in CPU Cgroups- How does Linux implement this? 
   
Notes (4/7):        
   - Dynamic allocation of matrices isn't totally necissary, but cool. Static alocation ok. 
               Good how it is now.
               Make both versions and compare the 2. 
   - Trace on kernelshark 
   - Play with both sched FIFO and both schedRR
                  first one runs to completion     2 processes switched out
         Use sched_setscheduler in server, to set it since threads will inherent it.
   - Linux CPU Cgroups to assign budgets for each client process
   - Don't need locks for shared memory, as thought

            Notes(3/31): Mark Bober (EIT) (Or Support@EIT) To run on LinuxLab
            Change signal handler to Real Time Signal
            Shared Memory for Matrix Coefficents (POSIX Shared Memory)
   In Future: Permissions of the shared memory so no one has capability to access, until given
   Marion: CH. 54 LPI (Scoped Shared Memory) --> Step Farther (CH.48 LPI) System 5 shared memory 
                                                         Access Control Lists -> Closer to Capability Model
