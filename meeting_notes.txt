

Meeting Notes (5/5/22):
   - Look into how cgroup v2 supports threads https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#threads
   - cgroup global
      -- Threaded -> server and all clients
   1.) Leaderboard table for identites and roles 
   2.) Admission control/ broker for client/server registration with cgroups created and associated and split/ merged accordingly 
   Uses:
      - elastic/mc/hierarchial CPU
      - access control (i.e. files)
      - Dedicated/ Shared Memory
      Ex: Camera takes pictures and can change image (i.e. matrix) size accordingly

Updates (5/4/22):
   - Fixed Dynamic Allocation bug, though omiited struct in favor of access to Pointer directly, using offset
   - Added option to use normal, naive MM or cache-optimal version (conditional compilation so that doesn't effect performance)
      - Issues with valgrind on Pi, so went with wrapper for CPU hardware performance counters
         - Tested on LinuxLab first as well using valgrind, then with perf tool
      - Not much cache performance difference as expected. Why?
   - Implemented client setup cgroup and server thread joins that cgroup
      when it connects, basicially inventing a new pattern.
	 How does deleting a cgroup work, or is it left to OS to cleanup?
         However, PID is the same between threads since we use pthread_create. 
         How to address this, or is it okay?   


Question (4/20/22): 
   - Parameters for SCHED_DEADLINE?? 
      - Wait for periodic timer version to see interaction 
   - Pointer use in shared memory??    [FIXED 5/3/22]
      - Don't use calloc- ftruncate and mmap with size of this.
   - What is point of cache optimal Matrix Multiply if we want workload? 
         Could use cachegrind tool available in valgrind
         Cache Memory 2 from CSE361S Slides
         Compare cache performance vs scheduling policy vs requests that come in 
         Gill: Single thread
         Marion: Performance monitoring from 522 Studio 3 to see cache hits/misses 
         

         Ended up using performance stats (which uses hardware performance counters of CPU) command: 
            https://stackoverflow.com/questions/10082517/simplest-tool-to-measure-c-program-cache-hit-miss-and-cpu-time-in-linux
            https://perf.wiki.kernel.org/index.php/Tutorial
            
         $ perf stat -e cache-misses -e cache-references ./cache_optimal_dense_mm 300

         Results for Original dense_mm for 500 size matrix:
               63,300,177      cache-misses:u            # 2.094 % of all cache refs
               3,023,069,371      cache-references:u

               8.898626738 seconds time elapsed

               8.861466000 seconds user
               0.030004000 seconds sys

         Results for Cache Optimal dense_mm for 500 size matrix:
               817,476      cache-misses:u            # 0.032 % of all cache refs
               2,524,564,128      cache-references:u

               5.119169061 seconds time elapsed

               5.092216000 seconds user
               0.020008000 seconds sys
       

         Results for Original dense_mm for 1000 size matrix:
               1,143,874,967      cache-misses:u        # 4.748 % of all cache refs
               24,092,088,964      cache-references:u

               87.931522410 seconds time elapsed

               87.771355000 seconds user
               0.070025000 seconds sys

         Results for Cache Optimal dense_mm for 1000 size matrix:
               7,030,832      cache-misses:u            # 0.035 % of all cache refs
               20,098,076,481      cache-references:u

               42.470107905 seconds time elapsed

               42.389324000 seconds user
               0.039989000 seconds sys


         Overall:
            ~ 60X Less Miss Rate for cache optimal versus original on 500 size matrix
            ~ 135X Less Miss Rate for cache optimal versus original on 1000 size matrix


Notes (4/14):
   - SCHED_DEADLINE add to flag set and see how it performs as well 
   - Change to dynamic allocation [DONE]
   - Possibly change matrix multiply to Cache-Optimal version 
   - Client is in a Cgroup, and Server thread joins that Cgroup when it links (inventing new pattern)
   - Server in own Cgroup, then above, and compare performance 
   - Marion: Hierarchy in CPU Cgroups- How does Linux implement this? 
   
Notes (4/7):        
   - Dynamic allocation of matrices isn't totally necissary, but cool. Static alocation ok. 
               Good how it is now.
               Make both versions and compare the 2. 
   - Trace on kernelshark 
   - Play with both sched FIFO and both schedRR
                  first one runs to completion     2 processes switched out
         Use sched_setscheduler in server, to set it since threads will inherent it.
   - Linux CPU Cgroups to assign budgets for each client process
   - Don't need locks for shared memory, as thought

            Notes(3/31): Mark Bober (EIT) (Or Support@EIT) To run on LinuxLab
            Change signal handler to Real Time Signal
            Shared Memory for Matrix Coefficents (POSIX Shared Memory)
   In Future: Permissions of the shared memory so no one has capability to access, until given
   Marion: CH. 54 LPI (Scoped Shared Memory) --> Step Farther (CH.48 LPI) System 5 shared memory 
                                                         Access Control Lists -> Closer to Capability Model
